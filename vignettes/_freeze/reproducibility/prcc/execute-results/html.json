{
  "hash": "d99370101381ca04a8a65cc21ab2a62b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Sensitivity analyses\"\nformat: html\n---\n\nThe list of packages required for reproducing the analyses on this script are listed in @lst-setup-sensitivity-analyses: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{#lst-setup-sensitivity-analyses .r .cell-code  lst-cap=\"Packages required for computing sensitivity analyses.\"}\n# remotes::install_github(\"jpahle/CoRC\")\nlibrary(CoRC)\n\n# tidyverse packages\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# to generate the simulations \nlibrary(lhs) # Latin Hypercube sampling\nlibrary(parallel) # parallel computing\nlibrary(ppcor) # Compute partial correlations\n\n# Table reporting\nlibrary(flextable)\n```\n:::\n\n\n## Evaluate the sensitivity of immune cells to parameter constants\n\n### Generate the distributions of nine critical parameters \n\nWe begin to extract the original constants of the 9 parameters assumed to have the strongest impact on the populations of T cell subsets, using `CoRC::getParameters`, reported in @tbl-healthy-parameters:  \n\n\n::: {#tbl-healthy-parameters .cell layout-align=\"center\" tbl-cap='The 9 reaction parameters evaluated for **sensitivity analyses**.'}\n\n```{.r .cell-code}\n## Retrieve parameters we want to evaluate the versatility ----\nhealthy_model <- loadModel(\"../../models/team_2016_final_model_lo2016.cps\")\n# S2 to be changed\nparameters_healthy <- getParameters(model = healthy_model,\n                                  key = c(\"(Diff of M0 to M1).delta_m_cit\",\n                                          \"(Diff of M1 to M2).sigma\",\n                                          \"(Induction of T1 from M).s12\",\n                                          \"(Proliferation of T1).s2\",\n                                          \"(Induction of T2).s4\",\n                                          \"(Induction of T17).s21\",\n                                          \"(Induction of T17).s6\",\n                                          \"(Induction of Tr).sb\",\n                                          \"(Induction of Tr).s10\")) |> \n  dplyr::select(-mapping)\n\nflextable(parameters_healthy) |> \n  add_footer_row(values = \"key is direct ID of the parameter in the model, reaction describes the biological mechanism associated with the value of the parameter, and value returns the constant value assumed for healthy individuals.\", \n                 colwidths = 4) |> \n  bold(part = \"header\") \n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"tabwid\"><style>.cl-0b682b58{}.cl-0b5c6778{font-family:'Arial';font-size:11pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-0b5c678c{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-0b6185f0{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-0b618604{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-0b61b570{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0b61b57a{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0b61b57b{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0b61b584{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0b61b585{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0b61b586{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0b61b58e{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table data-quarto-disable-processing='true' class='cl-0b682b58'><thead><tr style=\"overflow-wrap:break-word;\"><th class=\"cl-0b61b570\"><p class=\"cl-0b6185f0\"><span class=\"cl-0b5c6778\">key</span></p></th><th class=\"cl-0b61b570\"><p class=\"cl-0b6185f0\"><span class=\"cl-0b5c6778\">name</span></p></th><th class=\"cl-0b61b570\"><p class=\"cl-0b6185f0\"><span class=\"cl-0b5c6778\">reaction</span></p></th><th class=\"cl-0b61b57a\"><p class=\"cl-0b618604\"><span class=\"cl-0b5c6778\">value</span></p></th></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-0b61b57b\"><p class=\"cl-0b6185f0\"><span class=\"cl-0b5c678c\">(Diff of M0 to M1).delta_m_cit</span></p></td><td class=\"cl-0b61b57b\"><p class=\"cl-0b6185f0\"><span class=\"cl-0b5c678c\">delta_m_cit</span></p></td><td class=\"cl-0b61b57b\"><p class=\"cl-0b6185f0\"><span class=\"cl-0b5c678c\">Diff of M0 to M1</span></p></td><td class=\"cl-0b61b584\"><p class=\"cl-0b618604\"><span class=\"cl-0b5c678c\">2.40</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-0b61b57b\"><p class=\"cl-0b6185f0\"><span class=\"cl-0b5c678c\">(Diff of M1 to M2).sigma</span></p></td><td class=\"cl-0b61b57b\"><p class=\"cl-0b6185f0\"><span class=\"cl-0b5c678c\">sigma</span></p></td><td class=\"cl-0b61b57b\"><p class=\"cl-0b6185f0\"><span class=\"cl-0b5c678c\">Diff of M1 to M2</span></p></td><td class=\"cl-0b61b584\"><p class=\"cl-0b618604\"><span class=\"cl-0b5c678c\">24.00</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-0b61b57b\"><p class=\"cl-0b6185f0\"><span class=\"cl-0b5c678c\">(Induction of T1 from M).s12</span></p></td><td class=\"cl-0b61b57b\"><p class=\"cl-0b6185f0\"><span class=\"cl-0b5c678c\">s12</span></p></td><td class=\"cl-0b61b57b\"><p class=\"cl-0b6185f0\"><span class=\"cl-0b5c678c\">Induction of T1 from M</span></p></td><td class=\"cl-0b61b584\"><p class=\"cl-0b618604\"><span class=\"cl-0b5c678c\">10.93</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-0b61b57b\"><p class=\"cl-0b6185f0\"><span class=\"cl-0b5c678c\">(Proliferation of T1).s2</span></p></td><td class=\"cl-0b61b57b\"><p class=\"cl-0b6185f0\"><span class=\"cl-0b5c678c\">s2</span></p></td><td class=\"cl-0b61b57b\"><p class=\"cl-0b6185f0\"><span class=\"cl-0b5c678c\">Proliferation of T1</span></p></td><td class=\"cl-0b61b584\"><p class=\"cl-0b618604\"><span class=\"cl-0b5c678c\">1.23</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-0b61b57b\"><p class=\"cl-0b6185f0\"><span class=\"cl-0b5c678c\">(Induction of T2).s4</span></p></td><td class=\"cl-0b61b57b\"><p class=\"cl-0b6185f0\"><span class=\"cl-0b5c678c\">s4</span></p></td><td class=\"cl-0b61b57b\"><p class=\"cl-0b6185f0\"><span class=\"cl-0b5c678c\">Induction of T2</span></p></td><td class=\"cl-0b61b584\"><p class=\"cl-0b618604\"><span class=\"cl-0b5c678c\">1.94</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-0b61b57b\"><p class=\"cl-0b6185f0\"><span class=\"cl-0b5c678c\">(Induction of T17).s21</span></p></td><td class=\"cl-0b61b57b\"><p class=\"cl-0b6185f0\"><span class=\"cl-0b5c678c\">s21</span></p></td><td class=\"cl-0b61b57b\"><p class=\"cl-0b6185f0\"><span class=\"cl-0b5c678c\">Induction of T17</span></p></td><td class=\"cl-0b61b584\"><p class=\"cl-0b618604\"><span class=\"cl-0b5c678c\">156.17</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-0b61b57b\"><p class=\"cl-0b6185f0\"><span class=\"cl-0b5c678c\">(Induction of T17).s6</span></p></td><td class=\"cl-0b61b57b\"><p class=\"cl-0b6185f0\"><span class=\"cl-0b5c678c\">s6</span></p></td><td class=\"cl-0b61b57b\"><p class=\"cl-0b6185f0\"><span class=\"cl-0b5c678c\">Induction of T17</span></p></td><td class=\"cl-0b61b584\"><p class=\"cl-0b618604\"><span class=\"cl-0b5c678c\">156.17</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-0b61b57b\"><p class=\"cl-0b6185f0\"><span class=\"cl-0b5c678c\">(Induction of Tr).sb</span></p></td><td class=\"cl-0b61b57b\"><p class=\"cl-0b6185f0\"><span class=\"cl-0b5c678c\">sb</span></p></td><td class=\"cl-0b61b57b\"><p class=\"cl-0b6185f0\"><span class=\"cl-0b5c678c\">Induction of Tr</span></p></td><td class=\"cl-0b61b584\"><p class=\"cl-0b618604\"><span class=\"cl-0b5c678c\">14.02</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-0b61b585\"><p class=\"cl-0b6185f0\"><span class=\"cl-0b5c678c\">(Induction of Tr).s10</span></p></td><td class=\"cl-0b61b585\"><p class=\"cl-0b6185f0\"><span class=\"cl-0b5c678c\">s10</span></p></td><td class=\"cl-0b61b585\"><p class=\"cl-0b6185f0\"><span class=\"cl-0b5c678c\">Induction of Tr</span></p></td><td class=\"cl-0b61b586\"><p class=\"cl-0b618604\"><span class=\"cl-0b5c678c\">14.02</span></p></td></tr></tbody><tfoot><tr style=\"overflow-wrap:break-word;\"><td  colspan=\"4\"class=\"cl-0b61b58e\"><p class=\"cl-0b6185f0\"><span class=\"cl-0b5c678c\">key is direct ID of the parameter in the model, reaction describes the biological mechanism associated with the value of the parameter, and value returns the constant value assumed for healthy individuals.</span></p></td></tr></tfoot></table></div>\n```\n\n:::\n:::\n\n\n\nIn @lo2016po, they chose to perform **Latin Hypercube Sampling (LHS)** against **Standard Random Sampling**. The differences between these two randomisation approaches are further detailled in @tip-LHS, but briefly, the major advantage of LHS lies in its better coverage of the whole parameter sampling space, especially with a low number of observations (which is usually the case in boostrap simulations due to their high computing cost). \n\n::: {#tip-LHS .callout-tip collapse=\"true\" title=\"Standard versus Latin Hypercube Sampling (LHS)\"}\n\nRandom sampling and Latin Hypercube Sampling differ in how they distribute the sample points within the parameter space:\n\n---\n\nIn **Standard Random Sampling**, for a $d-$ random vector, the random samples are drawn independently within each dimension (or variable): \n\n$$\nx_i^{(j)} \\sim p_j(x), \\quad j = 1, 2, \\dots, d, \\quad i = 1, 2, \\dots, N\n$$\n\nwhere $x_i^{(j)}$ is the $i$-th sample drawn for the $j$-th dimension, $d=9$ (number of parameters evaluated for sensitivity), $N=5000$ the total number of parameter distributions simulated, and $p_j$ the probability distribution^[Only the uniform sampling, $U_i \\sim \\mathcal{U}(0,1)$, is provided with the `LHS`package. We detailed in @tip-ITST how to simulate any kind of probability distribution from the standard uniform distribution.].\nSince each sample is drawn completely at random, it's quite likely that some regions might be over-sampled while others may have gaps, with a low number of observations drawn (good coverage of the whole sampling space is only guaranteed asymptotically).\n\n--- \n\nIn **Latin Hypercube Sampling (LHS)**,  the sampling space is first stratified into $N$ equally probable intervals for each variable $x^{(j)}$; an independent permutation is subsequently applied across dimensions to avoid correlation. Finally, each sample is drawn from of the $N-$ defined intervals.\nLHS then guarantees a more evenly distributed coverage.\n\n---\n\nTo conclude, LHS provides a **better Coverage of the Input Space**(notably reduces the probability of clustering or missing critical regions), **Improved Convergence and Efficiency** by achieving a more accurate representation of the underlying distribution and leading to lower variance in estimated statistics with fewer number of avalaible observations (see @fig-LHS for further illustration). Yet, only standard random sampling is completely independent and unbiased. \n\n\n![Randomly sampled variables vs. Latin hypercube samples, from [@rustell](https://www.researchgate.net/figure/Randomly-sampled-variables-vs-Latin-hypercube-samples_fig3_322386503). With a low number of observations, the **clustering pattern** inherent to standard random sampling is clearly showcased.](../assets/img/Randomly-sampled-variables-vs-Latin-hypercube-samples.png){#fig-LHS}\n\n:::\n\nThe code snippet below generates the $N=5000$ bootstrap distribution of the 9 evaluated parameters for robustness, using **Latin Hypercube Sampling**. LHS indeed guarantees better coverage of the *sampling space*.^[Note however that both standard random sampling and LHS assume independence between the parameters values, a strong and quite unlikely biological premise.].\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn_samples <- 5000\n# Lower and upper bounds are computed within a ±20% range centered around the assumed parameter constant.\nparameters_sensitivity <- parameters_healthy$value\nparam_ranges <- tibble::tibble(parameters_name = parameters_healthy$key, \n                               LB= parameters_sensitivity *0.8,\n                               UB = parameters_sensitivity *1.2)\nset.seed(20) \nbootstrap_parameters <- randomLHS(n_samples, length(parameters_sensitivity))\n\nfor (i in seq_along(parameters_sensitivity)) {\n  bootstrap_parameters[,i] <- qunif(bootstrap_parameters[,i], \n                                    param_ranges$LB[i],\n                                    param_ranges$UB[i])\n}\ncolnames(bootstrap_parameters) <- parameters_healthy$name\napply(bootstrap_parameters,2, quantile)\n##       delta_m_cit    sigma       s12        s2       s4      s21       s6\n##  0%      1.920171 19.20104  8.744071 0.9840326 1.552041 124.9461 124.9454\n##  25%     2.160095 21.59964  9.837180 1.1070501 1.745990 140.5559 140.5551\n##  50%     2.399952 24.00043 10.930092 1.2299544 1.939971 156.1739 156.1738\n##  75%     2.639956 26.39955 12.022974 1.3529785 2.133962 171.7861 171.7831\n##  100%    2.879926 28.79855 13.115638 1.4759205 2.327949 187.3917 187.4018\n##             sb      s10\n##  0%   11.21699 11.21689\n##  25%  12.61806 12.61786\n##  50%  14.01999 14.01992\n##  75%  15.42221 15.42153\n##  100% 16.82292 16.82372\n```\n:::\n\n\n\n```r\nn_samples <- 5000 # <1>\n\nparameters_sensitivity <- parameters_healthy$value\nparam_ranges <- tibble::tibble(parameters_name = parameters_healthy$key, # <2>\n                               LB= parameters_sensitivity *0.8,          # <2>\n                               UB = parameters_sensitivity *1.2)         # <2>\n\n\nset.seed(20) # <3>\nbootstrap_parameters <- randomLHS(n_samples, length(parameters_sensitivity)) # <4>\n\n# \nfor (i in seq_along(parameters_sensitivity)) { # <5>\n  bootstrap_parameters[,i] <- qunif(bootstrap_parameters[,i], # <5>\n                                    param_ranges$LB[i],       # <5>\n                                    param_ranges$UB[i])       # <5>\n}                                                             # <5>\ncolnames(bootstrap_parameters) <- parameters_healthy$name\napply(bootstrap_parameters,2, quantile)                       # <6>\n```\n1. Determine the number of observations, equivalently the number of generated bootstrap distributions (here, $N=5000$).\n2. Lower and upper bounds are computed within a $\\pm 20\\%$ range, centered around the parameter constant.\n3. We fix the **random seed** to ensure reproducibibility of the sensitivity analyses.\n4. The core function of the `lhs` package, `lhs::randomLHS()`, is only able to perform standard uniform sampling. Two parameters must be provided: the number of observations to derive (here, 5000), and the dimension of the random vector (here, $9$ parameters are considered).\n5. How to generate the uniform distribution for a given parameter $X_j \\sim \\mathcal{U}(0.8 \\times X_j^0, 1.2 \\times  X_j^0)$, with $X_j^0$ the constant value estimated in a healthy individual, from the standard uniform distribution $U \\sim \\mathcal{U}(0, 1)$^[Note that we assume independence between the parameters, and that the *quantile call* is performed independenly for each of the dimensions.]. By appling the **Inverse Transform Sampling Theorem** (see @tip-ITST), which implies applying the reciprocal of the CDF, in other words, the *quantile function* which is given in R by the `stats::qunif` function. Alternatively, we could have reproduced this result by applying the following affine transformation: `bootstrap_parameters[,i] * (param_ranges$UB[i] -param_ranges$LB[i]) +  param_ranges$LB[i]` on the observations simulated wiht a standard uniform. \n6. By applying the `quantile` function, and since the distribution is supposed evenly distributed on the sampling space, we expect the quantiles to be rather close from their theoretical values (for example, both the median and the mean should be close to the real constant parameter values).\n\n\n::: {#tip-ITST .callout-tip collapse=\"true\" title=\"The Inverse Transform Sampling Theorem\"}\n\nThe **Inverse Transform Sampling Theorem** allows generating any probability distribution from the **standard uniform distribution**, $U \\sim \\mathcal{U}(0,1)$. Let $X$ be a continuous random variable with *cumulative distribution function* (CDF) $F(x)$ and $F^{-1}$ its reciprocal (the quantile function), then:\n\n$$\nX = F^{-1}(U)\n$$\n\nfollows the same distribution as $X$.\n\n---\n\nIn particular, for a uniform distribution $X \\sim \\mathcal{U}(a, b)$, $a$ and $b$ being respectively the lower and upper bounds, whose CDF is given by: $F(x) = \\frac{x - a}{b - a}, \\quad a \\leq x \\leq b$, then the *quantile* function is given by $F^{-1}(u) = a + u(b - a)$. \n\n\nIn other terms, if $U$ follows the standard uniform distirbution, then, by applying the **affine** transformation $X=a + (b - a) U$, $X$ will follow the $X \\sim \\mathcal{U}(a,b)$ distribution. \n\n\nGenerally speaking, the **Inverse Transform Sampling** allows generating any probability distribution from a uniform $[0,1]$ applying this formula: $X = F^{-1}(U)$.\n\n:::\n\n\n`bootstrap_parameters` is a 5000 simulations times 9 parameters matrix storing all sampled parameter configurations. We displayed below only the first six lines of the simulated parameter distributions:\n\n\n::: {.cell layout-align=\"center\"}\n\n```\n##       delta_m_cit    sigma       s12       s2       s4      s21       s6\n##  [1,]    1.959796 23.44807  9.274411 1.439972 2.220298 136.2398 149.3445\n##  [2,]    2.066597 19.27373  9.286449 1.204420 1.758831 127.5816 155.7143\n##  [3,]    2.436843 19.47846 12.596935 1.279335 2.209452 129.4354 159.1146\n##  [4,]    2.198476 22.11835 11.983510 1.453826 1.714273 144.2711 162.5585\n##  [5,]    2.138463 25.98209  9.325547 1.168549 2.253209 158.9362 172.0287\n##  [6,]    2.118051 22.26837  9.969860 1.435285 1.571099 126.8765 153.0293\n##             sb      s10\n##  [1,] 13.45400 15.11477\n##  [2,] 16.35203 13.94127\n##  [3,] 13.20491 14.63351\n##  [4,] 12.43537 14.92586\n##  [5,] 13.86563 15.75312\n##  [6,] 13.35385 11.89732\n```\n:::\n\n\n\n### Compute steady states for the 5000 simulated parameter variations\n\nWe compute the corresponding steady-states for each of the $N=5000$ simulated parameter configurations. This task is computationally-intensive, but can be easily parallelised, since all bootstrap samples have been generated independently. We use to that end the standard `parallel` package.\n\n```r\ncl <- makeCluster(detectCores()) # <1>\nclusterExport(cl, varlist = c(\"parameters_healthy\", \"bootstrap_parameters\")) # <2>\ncluster_environments <- clusterEvalQ(cl, { # <2>\n  library(CoRC) # <2>\n  library(dplyr, quietly = TRUE) # <2>\n  healthy_model <- loadModel(\"./models/team_2016_final_model_lo2016.cps\") # <2>\n}) # <2>\nsensitivity_outputs <- parLapply(cl = cl, # <3>\n                    X = 1:n_samples, # <3>\n                    f = function (i) { # <3>\n                      sensitivity_model <- healthy_model |> saveModelToString() |>  loadModelFromString()  # <5>\n                      setParameters(model = sensitivity_model, # <3>\n                                    key = parameters_healthy$key, # <3>\n                                    value = bootstrap_parameters[i,])  # <3>\n                      sensitivity_model_steady_state <- runSteadyState(  # <3>\n                        model = sensitivity_model # <3>\n                      )$species # <3>\n                      \n                      SS_concentrations <- setNames(sensitivity_model_steady_state$concentration,  # <3>\n                                                    sensitivity_model_steady_state$name) |>        # <3>\n                        as.list() |> as.data.frame() |>                                            # <3>\n                        dplyr::bind_cols(bootstrap_parameters[i, ,drop=FALSE] |>                   # <3>\n                                           as.data.frame())                                        # <3>\n                      return(SS_concentrations)                                                    # <3>\n                    }\n                    , chunk.size = 50) |>                                                         # <4>\n                    dplyr::bind_rows()  # <6>\n\nstopCluster(cl)  # <7>\n```\n1. Use `detectCores()` to determine the number of available CPU cores; conditionned to that number (usually 8 or 16 on recent personal laptop configurations); use `makeCluster()` to create the corresponding cluster^[With 8 cores, for instance, 8 computations can be performed in parallel. It might be advised to free completely a CPU to avoid computer freezing].\n2. Use `clusterExport()` to send current session variables to workers, and `clusterEvalQ()` to load required libraries and/or execute code prior to the parallelised computations. Both functions ensure that the cluster environement has the required modules to start the computation. \n3. The Parallel Processing `parLapply()` is the counterpart of the sequential `lapply()` function. The first argument `cl` is the cluster definition. For each parameter configuration among the $N=5000$, we modifify the original ODE model to reflect the LHS parameter configuration, then, compute the modified steady-states associated with that change of parameter configuration. Finally, we return for each parameter configuration a dataset with the modified 15 steady-state concentrations and the corresponding 9 parameter configurations. \n4. The `chunk.size` option controls how data is divided among workers Once computed, another batch of 50 parameter configurations is processed).\n5. This line is required, as `CoRC` uses copy by *reference* rather than by *value*. If we had just assigned the loaded healthy ODE model to the modified sensitivity model, `sensitivity_model <- healthy_model`, every modification on `sensitivity_model` would also be reported on `healthy_model`^[Another way to visualise the **copy by reference** approach: simply copy the ODE model in another variable `healthy_model_copy <- healthy_model` in the R canonical way. Compare the storage addresses: `lobstr::obj_addr(healthy_model)` and `lobstr::obj_addr(healthy_model_copy)`; you will note that they are identical].\n6. The output of `parLapply` is a list of `data.frame`, function `dplyr::bind_rows()` concatenates them in a single large dataset.\n7. Stop the cluster using `stopCluster()` to free computational resources when no longer need.\n\n### Compute partial correlation scores\n\nTo compute the correlations between variations of the constant parameters, and the concentrations of the species at steady state, @lo2016po chose to apply a *rank-transformation* first, which guarantees increased robustness to outliers, and reduces *scaling issues* associated with significant variations of orders of magnitude between the parameters.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# reshape and rank-transform sensitivity outputs\nsensitivity_outputs <- readr::read_csv(\"../../results/sensitivity_outputs.csv\") |>\n  dplyr::mutate(across(where(is.numeric), rank)) \n```\n:::\n\n\n\nThen, instead on relying on standard Pearson correlation which may yield spurious relationships between outcomes and parameter variations, @lo2016po computes the **Partial Correlation** score between the rank-transformed variations of species of interest, and the simulated parameter distributions (see @imp-prcc for details). \n\nThe `pcor::ppcor` function returns both:\n\n- `estimate`, the symmetric matrix of the partial correlation coefficient between two variables (included between -1 and 1, the sign providing the type of correlation, and the absolute value the magnitude or strenght of the direct relationship)\n- `p.value`, a matrix of the $p-$ values for each of the tests. \n- As in @lo2016po, the threshold for *significance* was set to $0.01$.\n\nThe resulting PRCC scores between T1 and T2 immune subpopulations with the 9 parameters tested for sensitivity are reported in @tbl-pcor-computation:\n\n\n::: {#tbl-pcor-computation .cell layout-align=\"center\" tbl-cap='PRCC values of the 9 parameters benchmarked for **robustness** against steady-state concentrations of $T_1$ and $T_2$ immune cell subtypes. Reproduction of Table 5 in @lo2016po.'}\n\n```{.r .cell-code}\nprcc_sensitivity <- pcor(sensitivity_outputs)\nprcc_estimate <- prcc_sensitivity$estimate |> \n  tibble::as_tibble(mat, rownames = \"species\") |> \n  dplyr::select (species, T1, T2) |> \n  filter(!species %in% c(\"T1\", \"T2\")) |> \n  # Join on the same dataset pvalues and PRCC estimations.\n  inner_join(prcc_sensitivity$p.value |> \n               tibble::as_tibble(mat, rownames = \"species\") |> \n               dplyr::select (species, T1, T2) |> \n               filter(!species %in% c(\"T1\", \"T2\")), by = \"species\",\n             suffix = c(\"\", \".pval\")) |> \n  mutate(T1 = if_else(T1.pval <=0.01, sprintf(\"%.3f*\", T1), sprintf(\"%.3f\", T1)),\n         T2 = if_else(T2.pval <=0.01, sprintf(\"%.3f*\", T2), sprintf(\"%.3f\", T2))) |> \n  dplyr::select(-T1.pval, -T2.pval)\n\n### Format PRCC table\nflextable(prcc_estimate) |> \n  add_footer_row(values = \"* denotes significant PRCC with p-value below 0.01.\", \n                 colwidths = 3) |> \n  compose(j = \"species\",\n          value = as_paragraph(as_equation(c(\"\\\\sigma_{M_{\\\\alpha}}\", \"\\\\sigma_{M_{10}}\",\n                                             \"\\\\sigma_{12}\", \"\\\\sigma_{2}\", \"\\\\sigma_{4}\",\n                                             \"\\\\sigma_{21}\", \"\\\\sigma_{6}\", \n                                             \"\\\\sigma_{\\\\beta}\", \"\\\\sigma_{10}\")))) |> \n  set_header_labels(values = list(\n    species = \"\", T1 = \"PRCC for T1\", T2 = \"PRCC for T2\")) |> \n  align(align = \"center\", part = \"all\") |> \n  set_caption(\"Table 5: PRCC values for key parameters playing on T1 and T2 variations.\") \n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"tabwid\"><style>.cl-0d33fc32{}.cl-0d1db24c{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-0d27aab8{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-0d28067a{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0d28068e{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0d28068f{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0d280698{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table data-quarto-disable-processing='true' class='cl-0d33fc32'><thead><tr style=\"overflow-wrap:break-word;\"><th class=\"cl-0d28067a\"><p class=\"cl-0d27aab8\"><span class=\"cl-0d1db24c\"></span></p></th><th class=\"cl-0d28067a\"><p class=\"cl-0d27aab8\"><span class=\"cl-0d1db24c\">PRCC for T1</span></p></th><th class=\"cl-0d28067a\"><p class=\"cl-0d27aab8\"><span class=\"cl-0d1db24c\">PRCC for T2</span></p></th></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-0d28068e\"><p class=\"cl-0d27aab8\">NA</p></td><td class=\"cl-0d28068e\"><p class=\"cl-0d27aab8\"><span class=\"cl-0d1db24c\">0.918*</span></p></td><td class=\"cl-0d28068e\"><p class=\"cl-0d27aab8\"><span class=\"cl-0d1db24c\">0.364*</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-0d28068e\"><p class=\"cl-0d27aab8\">NA</p></td><td class=\"cl-0d28068e\"><p class=\"cl-0d27aab8\"><span class=\"cl-0d1db24c\">-0.351*</span></p></td><td class=\"cl-0d28068e\"><p class=\"cl-0d27aab8\"><span class=\"cl-0d1db24c\">0.429*</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-0d28068e\"><p class=\"cl-0d27aab8\">NA</p></td><td class=\"cl-0d28068e\"><p class=\"cl-0d27aab8\"><span class=\"cl-0d1db24c\">0.967*</span></p></td><td class=\"cl-0d28068e\"><p class=\"cl-0d27aab8\"><span class=\"cl-0d1db24c\">0.051*</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-0d28068e\"><p class=\"cl-0d27aab8\">NA</p></td><td class=\"cl-0d28068e\"><p class=\"cl-0d27aab8\"><span class=\"cl-0d1db24c\">0.876*</span></p></td><td class=\"cl-0d28068e\"><p class=\"cl-0d27aab8\"><span class=\"cl-0d1db24c\">0.024</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-0d28068e\"><p class=\"cl-0d27aab8\">NA</p></td><td class=\"cl-0d28068e\"><p class=\"cl-0d27aab8\"><span class=\"cl-0d1db24c\">-0.067*</span></p></td><td class=\"cl-0d28068e\"><p class=\"cl-0d27aab8\"><span class=\"cl-0d1db24c\">0.920*</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-0d28068e\"><p class=\"cl-0d27aab8\">NA</p></td><td class=\"cl-0d28068e\"><p class=\"cl-0d27aab8\"><span class=\"cl-0d1db24c\">-0.007</span></p></td><td class=\"cl-0d28068e\"><p class=\"cl-0d27aab8\"><span class=\"cl-0d1db24c\">0.019</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-0d28068e\"><p class=\"cl-0d27aab8\">NA</p></td><td class=\"cl-0d28068e\"><p class=\"cl-0d27aab8\"><span class=\"cl-0d1db24c\">0.021</span></p></td><td class=\"cl-0d28068e\"><p class=\"cl-0d27aab8\"><span class=\"cl-0d1db24c\">-0.007</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-0d28068e\"><p class=\"cl-0d27aab8\">NA</p></td><td class=\"cl-0d28068e\"><p class=\"cl-0d27aab8\"><span class=\"cl-0d1db24c\">-0.132*</span></p></td><td class=\"cl-0d28068e\"><p class=\"cl-0d27aab8\"><span class=\"cl-0d1db24c\">0.084*</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-0d28068f\"><p class=\"cl-0d27aab8\">NA</p></td><td class=\"cl-0d28068f\"><p class=\"cl-0d27aab8\"><span class=\"cl-0d1db24c\">-0.062*</span></p></td><td class=\"cl-0d28068f\"><p class=\"cl-0d27aab8\"><span class=\"cl-0d1db24c\">0.087*</span></p></td></tr></tbody><tfoot><tr style=\"overflow-wrap:break-word;\"><td  colspan=\"3\"class=\"cl-0d280698\"><p class=\"cl-0d27aab8\"><span class=\"cl-0d1db24c\">* denotes significant PRCC with p-value below 0.01.</span></p></td></tr></tfoot></table></div>\n```\n\n:::\n:::\n\n\n\n::: {#imp-prcc .callout-important collapse=\"true\" title=\"Pearson Correlation and Partial Correlation\"}\n\nThe standard **Pearson correlation** coefficient between two variables $X$ and $Y$ is given by @eq-pearson-correlation:  \n\n$$\n\\rho_{X,Y} = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\n$${#eq-pearson-correlation}\n\nwhere:  \n- $\\text{Cov}(X, Y) = \\mathbb{E}[(X - \\mu_X)(Y - \\mu_Y)]$ is the covariance.  \n- $\\sigma_X = \\sqrt{\\text{Var}(X)}$ and $\\sigma_Y = \\sqrt{\\text{Var}(Y)}$ are the variable standard deviations. \n\nIt measures the **linear relationship** between $X$ and $Y$, but does not control the effect of potential colliders or confounding variables.\n\n---\n\nOn the other hand, the *Partial Correlation* measures the relationship between two variables while **controlling for** the effect of a the remaining set of variables. In a multi-dimensional framework, it's given by @eq-partial-correlation:\n\n$$\n\\rho_{X,Y \\mid \\mathbf{Z}} = -\\frac{\\Omega_{X,Y}}{\\sqrt{\\Omega_{X,X} \\Omega_{Y,Y}}}\n$${#eq-partial-correlation}\n\nwhere $\\mathbf{\\Omega} = \\mathbf{\\Sigma}^{-1}$ is the **precision matrix** (inverse of the covariance matrix).  \n\n---\n\nCompared to standard correlations, it removes indirect Relationships triggered by another set of confusing **variables** $\\mathbf{Z}$, thus isolates the **direct** relationship between $X$ and $Y$, and enables more straightforward interpretation by revealing truly causal relationships. \nFor example, in @fig-prcc-illustration, using simply the Pearson correlation, we would have certainly observed a significant correlation between variables $A$ and $C$. On the other hand, with partial correlation, the spurious connection between $A$ and $C$ due to the confusing effect of $B$ would certainly has been faded out. \n\n```{dot}\n//| label: fig-prcc-illustration\n//| fig-cap: \"Simple example of **chain rule** associations.\"\ndigraph G {\n    layout=neato\n    A -> B;\n    B -> C;\n}\n```\n\n:::\n\n### Plot rank-transformed datasets\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsensitivity_outputs_formatted <- sensitivity_outputs |>\n  # tidy format: one column storing PRCC values for the x-axis, one for the y-axis, and another for the facets\n  dplyr::select (c(\"T1\", \"T2\", \"sb\", \"s10\", \"s12\")) |> \n  tidyr::pivot_longer(cols = c(\"sb\", \"s10\", \"s12\"), names_to = \"parameters\", values_to = \"x\") |>\n  tidyr::pivot_longer(cols = c(\"T1\", \"T2\"), names_to = \"species\", values_to = \"y\") |> \n  # provide the same order of facets for direct comparison\n  dplyr::mutate(parameters = factor(parameters, \n                                    levels = c(\"sb\", \"s10\", \"s12\"),\n                                    labels = c(\"sigma[b]\", \"sigma[10]\", \"sigma[12]\"),\n                                    ordered = TRUE),\n                species = factor(species, \n                                 levels = c(\"T1\", \"T2\"),\n                                 labels = c(\"T[1]\",\"T[2]\")))\n\n# Subtitles customised for each facet, including the p-value.\nprcc_values <- prcc_sensitivity$estimate\nprcc_sensitivity_annot <- data.frame(x=n_samples/2, y=Inf, \n                                     lab=factor(paste(\"PRCC =\", c(prcc_values[\"T1\", \"sb\"],\n                                                           prcc_values[\"T1\", \"s10\"],\n                                                           prcc_values[\"T1\", \"s12\"],\n                                                           prcc_values[\"T2\", \"sb\"],\n                                                           prcc_values[\"T2\", \"s10\"],\n                                                           prcc_values[\"T2\", \"s12\"]) |> \n                                                 sprintf(fmt = '%.3f')), ordered = TRUE), \n                                     parameters=factor(rep(c(\"sigma[b]\", \"sigma[10]\", \"sigma[12]\"), 2), \n                                                       ordered = TRUE),\n                                     species= factor(c(rep(\"T[1]\", 3), rep(\"T[2]\", 3)), \n                                                     ordered = TRUE))\n```\n:::\n\n\nWe generate a scatter plot after applying rank-transformation of $T_1$ and $T_2$ immune cells subsets, against 3 parameters playing a critical role on the viability of cell pools, namely $\\sigma_b$, $\\sigma_{10}$ and $\\sigma_{12}$, the first two controlling the activation rate of Tregs, and the induction of $T_1$ by the global pool of macrophages (including both $M1$ and $M2$ types)^[Note that we don't round the PRCC values, but only format them to display 3 significant digits using the native C `sprintf()`: difference between both approaches is notably crucial when saving datasets to avoid loss of nuemrical precision].\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nSS_plots <- ggplot(sensitivity_outputs_formatted, aes(y = y, x=x)) +\n  geom_point(size = 0.1, col = \"blue\", shape = 20) +\n  # The labeller trick enables to use Latex notations for the subtitles of facets.\n  facet_grid(species ~ parameters, \n             labeller = labeller(.rows = label_parsed, .cols = label_parsed)) +\n  geom_label(aes(x, y, label=lab),\n            data=prcc_sensitivity_annot, vjust=0.8, size = 3) +\n  xlab(\"Cell Species\") + ylab(\"Free and Variable Parameters\") +\n  # Guarantees minimal plot background that would hinder visibility of the output.\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), \n        panel.grid.minor = element_blank(),\n        strip.text.y = element_text(angle = 0)) \nSS_plots\n```\n\n::: {.cell-output-display}\n![Scatter plots of *rank-transformed* $T_1$ and $T_2$ against 3 parameters enumerated in @tbl-pcor-computation, exhbiting both strong partial correlation ($|\\text{PRCC}|>0.5$ and $p-$value $<0.01$). Reproduction of [@lo2016po, pp. 11], Fig.2.](prcc_files/figure-html/fig-sensitivity-plots-1.png){#fig-sensitivity-plots fig-align='center' width=95%}\n:::\n:::\n\n\n\nOne of the major advantages of the `ggplot2` package lies in its seamless ability to save the graphical object in a variety of formats, including `pdf` and `png`, as illustrated in @lst-ggplot-save: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{#lst-ggplot-save .r .cell-code  lst-cap=\"Code snippet not run, used for illustration example.\"}\n# output is pdf, with a high resolution close to retina visual discrimination ability\nggsave(filename = \"../../results/Fig2_PRCC.pdf\", \n       plot = SS_plots, dpi = 600, width = 20, height = 10,\n       units = \"cm\")\n```\n:::\n\n",
    "supporting": [
      "prcc_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/tabwid-1.1.3/tabwid.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/tabwid-1.1.3/tabwid.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}